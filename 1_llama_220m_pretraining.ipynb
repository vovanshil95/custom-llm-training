{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "068fc93d-3112-4445-9bf4-a48fb996467c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 13:07:55.935912: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742292475.958292  559166 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742292475.965996  559166 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-18 13:07:55.990435: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaConfig\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import LlamaForCausalLM\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fd0ed04-35ac-4006-8698-854b7bace718",
   "metadata": {},
   "outputs": [],
   "source": [
    "for device in range(torch.cuda.device_count()):\n",
    "    torch.cuda.set_device(device)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "526dda7b-cc16-47af-abf6-9c31ac3637df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af21b4238e24488ea9903a01b525e0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c1b2cb2-0b7f-480a-9ab9-81a88fb41cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvladimirshilonosov2\u001b[0m (\u001b[33mvladimirshilonosov2-itmo\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2307bd3-1c5f-4398-9e21-ebda27ba9a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ab82f1e2b44b15a205655851be49db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f40bf5039f462e9a047c9aebec9ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"danasone/wikipedia_ru\", split=\"train\")\n",
    "\n",
    "\n",
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000 * 100):\n",
    "        yield dataset[i : i + 1000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "927c0a1f-20ec-463f-985b-4e5def7f84f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|█████████████████████| 21/21 [00:00<00:00, 48.87it/s]\n",
      "Loading dataset shards: 100%|█████████████████| 21/21 [00:00<00:00, 1533.19it/s]\n",
      "\u001b[2K[00:00:12] Tokenize words                 ██████████████████ 8175842  /  8175842\n",
      "\u001b[2K[00:00:31] Count pairs                    ██████████████████ 8175842  /  8175842\n",
      "\u001b[2K[00:01:14] Compute merges                 ██████████████████ 29944    /    29944\n"
     ]
    }
   ],
   "source": [
    "!python train_tokenizer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f95d8f94-7b23-469b-9c11-c52c992207e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file='custom_ru_tokenizer.json'\n",
    ")\n",
    "special_tokens = {\n",
    "    \"bos_token\": \"<|bos|>\",\n",
    "    \"eos_token\": \"<|eos|>\",\n",
    "    \"unk_token\": \"<|unk|>\",\n",
    "    \"pad_token\": \"<|pad|>\",\n",
    "    \"mask_token\": \"<|mask|>\",\n",
    "    \"additional_special_tokens\": [\"<|user|>\", \"<|bot|>\", \"<|end|>\"]\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca2c6f16-cfb7-45e7-a3f0-be637336b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_PART_SIZE = 51\n",
    "CONTEXT_SIZE = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b59d0c18-d3af-4fb5-b260-666cc83be159",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_config = LlamaConfig(\n",
    "    vocab_size=32000,\n",
    "    hidden_size=896,\n",
    "    intermediate_size=3584,\n",
    "    num_hidden_layers=16,\n",
    "    num_attention_heads=16,\n",
    "    num_key_value_heads=8,\n",
    "    max_position_embeddings=CONTEXT_SIZE,\n",
    "    rope_theta=10000.0,\n",
    "    attention_bias=False,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    tie_word_embeddings=True,\n",
    "    initializer_range=1.5e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1ae12c6-7ee5-4dfb-8bc6-73a145b0d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaForCausalLM(custom_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d8a27b4-968d-4a5f-ae04-ec3ebccffe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Параметров модели: 221,377,408\n"
     ]
    }
   ],
   "source": [
    "print(f\"Параметров модели: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be055d55-3c5c-4b1f-a60d-0ed2158e40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_small_parts(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=SMALL_PART_SIZE,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "def tokenize(element):\n",
    "    max_length = CONTEXT_SIZE - 256 - 2\n",
    "    \n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=False,\n",
    "        return_length=True,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "\n",
    "    out_batch = []\n",
    "\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length <= max_length:\n",
    "            out_batch.append([tokenizer.bos_token_id] + input_ids.tolist() + [tokenizer.eos_token_id])\n",
    "            \n",
    "    return {\"input_ids\": out_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beabf53-ed1d-4a28-84f1-b271776ce622",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    tokenize, \n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    "    num_proc=16\n",
    ")\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9dc799f-0942-46a1-9e11-3801b447e018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 1791988\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 94316\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = tokenized_dataset.train_test_split(\n",
    "    test_size=0.05,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c71f5c8-e50f-4f12-9020-51169bdc830c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8f5e3e6796437e83e4cd94ea322b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/8 shards):   0%|          | 0/1791988 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8977dee0bbbb4cf18a0cfd74d56b3b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/94316 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset.save_to_disk(\"tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50a98b9e-2bd8-4729-b251-e22b5d3695ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = load_from_disk('tokenized_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e726af44-062f-4c73-9998-0f520bf9033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffSizeDataCollator(DataCollatorForLanguageModeling):\n",
    "    def __init__(self, tokenizer, padding_side='left', pad_to_multiple_of=8, **kwargs):\n",
    "        super().__init__(tokenizer, pad_to_multiple_of=pad_to_multiple_of, **kwargs)\n",
    "        self.padding_side = padding_side\n",
    "        self.pad_to_multiple_of = pad_to_multiple_of\n",
    "\n",
    "    def __call__(self, features):\n",
    "        max_length = max(len(f['input_ids']) for f in features)\n",
    "        \n",
    "        if self.pad_to_multiple_of is not None:\n",
    "            padded_length = ((max_length + self.pad_to_multiple_of - 1) // self.pad_to_multiple_of) * self.pad_to_multiple_of\n",
    "        else:\n",
    "            padded_length = max_length\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding='longest',\n",
    "            pad_to_multiple_of=padded_length,\n",
    "            return_tensors='pt',\n",
    "            padding_side=self.padding_side\n",
    "        )\n",
    "        \n",
    "        labels = batch['input_ids'].clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        batch['labels'] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b11fff17-752d-49e9-92ca-8bdeaeb8861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DiffSizeDataCollator(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8fcee78-4097-4b01-bec3-f3c8d63c2f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([8, 3024])\n",
      "attention_mask shape: torch.Size([8, 3024])\n",
      "labels shape: torch.Size([8, 3024])\n"
     ]
    }
   ],
   "source": [
    "out = data_collator([tokenized_dataset['train'][i] for i in range(8)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7440e2e1-2a36-4174-99cd-12bfb635e16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-18 13:09:03,092] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vshilonosov/miniconda3/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/vshilonosov/miniconda3/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"Llama-ru-220M\",\n",
    "    hub_model_id=\"NLPVladimir/Llama-ru-220M\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.001,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    learning_rate=1e-3,\n",
    "    save_steps=100,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    "    run_name='Llama-ru-220M_pretraining',\n",
    "    report_to=\"wandb\",\n",
    "    optim=\"sgd\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45558446-3cbd-4561-98f1-72660e9a7e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvladimirshilonosov2\u001b[0m (\u001b[33mvladimirshilonosov2-itmo\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/vshilonosov/custom-llm-training/wandb/run-20250318_130906-dwh17hd2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vladimirshilonosov2-itmo/huggingface/runs/dwh17hd2' target=\"_blank\">Llama-ru-220M_pretraining</a></strong> to <a href='https://wandb.ai/vladimirshilonosov2-itmo/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vladimirshilonosov2-itmo/huggingface' target=\"_blank\">https://wandb.ai/vladimirshilonosov2-itmo/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vladimirshilonosov2-itmo/huggingface/runs/dwh17hd2' target=\"_blank\">https://wandb.ai/vladimirshilonosov2-itmo/huggingface/runs/dwh17hd2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vshilonosov/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='149332' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    17/149332 00:46 < 128:42:28, 0.32 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:2162\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2160\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   2161\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 2162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2163\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2164\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   2165\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   2166\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2167\u001b[0m     )\n\u001b[1;32m   2168\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2169\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:2536\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2540\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df56714a-27e6-4037-ab81-f2ba6739428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
