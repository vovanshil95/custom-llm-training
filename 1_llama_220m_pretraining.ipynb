{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "068fc93d-3112-4445-9bf4-a48fb996467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaConfig\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import LlamaForCausalLM\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fd0ed04-35ac-4006-8698-854b7bace718",
   "metadata": {},
   "outputs": [],
   "source": [
    "for device in range(torch.cuda.device_count()):\n",
    "    torch.cuda.set_device(device)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "526dda7b-cc16-47af-abf6-9c31ac3637df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132b04ff1f3548c2bfd5ca256a2f1a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c1b2cb2-0b7f-480a-9ab9-81a88fb41cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvladimirshilonosov2\u001b[0m (\u001b[33mvladimirshilonosov2-itmo\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2307bd3-1c5f-4398-9e21-ebda27ba9a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9191301225430c8a382ebd108f4f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135b37e4dca7463f85692bd96517c155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"danasone/wikipedia_ru\", split=\"train\")\n",
    "\n",
    "\n",
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000 * 100):\n",
    "        yield dataset[i : i + 1000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927c0a1f-20ec-463f-985b-4e5def7f84f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|█████████████████████| 21/21 [00:00<00:00, 52.63it/s]\n",
      "Loading dataset shards: 100%|█████████████████| 21/21 [00:00<00:00, 1518.42it/s]\n",
      "\u001b[2K[00:00:57] Pre-processing sequences       █░░░░░░░░░░░░░░░░░ 147428   /  1925386"
     ]
    }
   ],
   "source": [
    "!python train_tokenizer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d8f94-7b23-469b-9c11-c52c992207e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file='custom_ru_tokenizer.json'\n",
    ")\n",
    "special_tokens = {\n",
    "    \"bos_token\": \"<|bos|>\",\n",
    "    \"eos_token\": \"<|eos|>\",\n",
    "    \"unk_token\": \"<|unk|>\",\n",
    "    \"pad_token\": \"<|pad|>\",\n",
    "    \"mask_token\": \"<|mask|>\",\n",
    "    \"additional_special_tokens\": [\"<|user|>\", \"<|bot|>\", \"<|end|>\"]\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c6f16-cfb7-45e7-a3f0-be637336b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_PART_SIZE = 512\n",
    "CONTEXT_SIZE = 4096\n",
    "MAX_TRAIN_LENGTH = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d0c18-d3af-4fb5-b260-666cc83be159",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_config = LlamaConfig(\n",
    "    vocab_size=32000,\n",
    "    hidden_size=896,\n",
    "    intermediate_size=3584,\n",
    "    num_hidden_layers=16,\n",
    "    num_attention_heads=16,\n",
    "    num_key_value_heads=8,\n",
    "    max_position_embeddings=CONTEXT_SIZE,\n",
    "    rope_theta=10000.0,\n",
    "    attention_bias=False,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    tie_word_embeddings=True,\n",
    "    initializer_range=1.5e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ae12c6-7ee5-4dfb-8bc6-73a145b0d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaForCausalLM(custom_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8a27b4-968d-4a5f-ae04-ec3ebccffe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Параметров модели: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be055d55-3c5c-4b1f-a60d-0ed2158e40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_small_parts(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=SMALL_PART_SIZE,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "def tokenize(element):\n",
    "    \n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=False,\n",
    "        return_length=True,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "\n",
    "    out_batch = []\n",
    "\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length <= MAX_TRAIN_LENGTH:\n",
    "            out_batch.append([tokenizer.bos_token_id] + input_ids.tolist() + [tokenizer.eos_token_id])\n",
    "            \n",
    "    return {\"input_ids\": out_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beabf53-ed1d-4a28-84f1-b271776ce622",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    tokenize, \n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    "    num_proc=16\n",
    ")\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dc799f-0942-46a1-9e11-3801b447e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.train_test_split(\n",
    "    test_size=0.05,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c71f5c8-e50f-4f12-9020-51169bdc830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.save_to_disk(\"tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a98b9e-2bd8-4729-b251-e22b5d3695ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = load_from_disk('tokenized_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f297143-37ea-4288-872b-d9a4d9009b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "946d6250-d978-47d5-84e1-63a7fb162a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tokenized_dataset[\"test\"].map(\n",
    "    lambda example: {\"num_tokens\": len(example[\"input_ids\"])},\n",
    "    batched=False,\n",
    "    num_proc=16\n",
    ")\n",
    "sorted_test_dataset = test_dataset.sort(\"num_tokens\", reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e726af44-062f-4c73-9998-0f520bf9033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffSizeDataCollator(DataCollatorForLanguageModeling):\n",
    "    def __init__(self, tokenizer, padding_side='left', pad_to_multiple_of=8, **kwargs):\n",
    "        super().__init__(tokenizer, pad_to_multiple_of=pad_to_multiple_of, **kwargs)\n",
    "        self.padding_side = padding_side\n",
    "        self.pad_to_multiple_of = pad_to_multiple_of\n",
    "\n",
    "    def __call__(self, features):\n",
    "        max_length = max(len(f['input_ids']) for f in features)\n",
    "        \n",
    "        if self.pad_to_multiple_of is not None:\n",
    "            padded_length = ((max_length + self.pad_to_multiple_of - 1) // self.pad_to_multiple_of) * self.pad_to_multiple_of\n",
    "        else:\n",
    "            padded_length = max_length\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding='longest',\n",
    "            pad_to_multiple_of=padded_length,\n",
    "            return_tensors='pt',\n",
    "            padding_side=self.padding_side\n",
    "        )\n",
    "        \n",
    "        labels = batch['input_ids'].clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        batch['labels'] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbe20616-4d76-4eba-ae2a-c76ecb4a4771",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchEvalTrainer(Trainer):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def evaluate(self, ignore_keys):\n",
    "\n",
    "        dataloader = self.get_eval_dataloader()\n",
    "\n",
    "        model = self._wrap_model(self.model, training=False, dataloader=dataloader)\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        progress_bar = tqdm(\n",
    "            total=len(dataloader),\n",
    "            desc=\"Evaluation\",\n",
    "            unit=\"batch\",\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "\n",
    "        losses = []\n",
    "        \n",
    "        accs = {\n",
    "            'accuracy': [],\n",
    "            'sum_tokens': []\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for step, batch in enumerate(dataloader):\n",
    "                outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "    \n",
    "                batch_preds = []\n",
    "                batch_labels = []\n",
    "    \n",
    "                for seq_ids, seq_logits in zip(batch[\"input_ids\"], outputs.logits):\n",
    "                    \n",
    "                    unpadded_seq_logits = seq_logits[seq_ids != self.processing_class.pad_token_id]\n",
    "                    unpadded_seq_ids = seq_ids[seq_ids != self.processing_class.pad_token_id]\n",
    "                    unpadded_preds = torch.argmax(unpadded_seq_logits, dim=-1)\n",
    "    \n",
    "                    batch_preds.append(unpadded_preds[:-1])\n",
    "                    batch_labels.append(unpadded_seq_ids[1:])\n",
    "    \n",
    "                batch_preds = torch.cat(batch_preds)\n",
    "                batch_labels = torch.cat(batch_labels)\n",
    "                \n",
    "                accuracy = (batch_preds == batch_labels).float().mean()\n",
    "                    \n",
    "                accs['accuracy'].append(accuracy)\n",
    "                accs['sum_tokens'].append(len(batch_labels))\n",
    "                \n",
    "                losses.append(outputs.loss.cpu())\n",
    "    \n",
    "                \n",
    "                progress_bar.set_postfix({\n",
    "                    \"step\": step+1\n",
    "                })\n",
    "                \n",
    "                progress_bar.update(1)\n",
    "    \n",
    "            progress_bar.close()\n",
    "                \n",
    "            loss = torch.mean(torch.cat(losses))\n",
    "            accuracy = torch.sum(torch.Tensor(accs['accuracy']) * torch.Tensor(accs['sum_tokens'])) / sum(accs['sum_tokens'])\n",
    "            \n",
    "            try:\n",
    "                perplexity = torch.exp(loss)\n",
    "            except OverflowError:\n",
    "                perplexity = float(\"inf\")\n",
    "    \n",
    "            metrics = {'eval_loss': loss.item(), 'eval_perplexity': perplexity.item(), 'eval_accuracy': accuracy.item()} \n",
    "            \n",
    "        self.log(metrics) \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b11fff17-752d-49e9-92ca-8bdeaeb8861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DiffSizeDataCollator(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8fcee78-4097-4b01-bec3-f3c8d63c2f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([8, 1032])\n",
      "attention_mask shape: torch.Size([8, 1032])\n",
      "labels shape: torch.Size([8, 1032])\n"
     ]
    }
   ],
   "source": [
    "out = data_collator([tokenized_dataset['train'][i] for i in range(8)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7440e2e1-2a36-4174-99cd-12bfb635e16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-23 03:12:19,612] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vshilonosov/miniconda3/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/vshilonosov/miniconda3/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "GPU_COUNT = 3\n",
    "BATCH_PER_GPU_TRAIN = 1\n",
    "BATCH_PER_GPU_TEST = 2\n",
    "STEPS_TO_UPDATE = 512\n",
    "EVALS_PER_EPOCH = 25\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"Llama-ru-220M\",\n",
    "    hub_model_id=\"NLPVladimir/Llama-ru-220M\",\n",
    "    per_device_train_batch_size=BATCH_PER_GPU_TRAIN,\n",
    "    per_device_eval_batch_size=BATCH_PER_GPU_TEST,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=int(len(tokenized_dataset['train']) / STEPS_TO_UPDATE / EVALS_PER_EPOCH),\n",
    "    logging_steps=1,\n",
    "    gradient_accumulation_steps=int(STEPS_TO_UPDATE / BATCH_PER_GPU_TRAIN / GPU_COUNT),\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.001,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    learning_rate=1e-3,\n",
    "    save_steps=int(len(tokenized_dataset['train']) / STEPS_TO_UPDATE / EVALS_PER_EPOCH),\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    push_to_hub=True,\n",
    "    run_name='Llama-ru-220M_pretraining',\n",
    "    report_to=\"wandb\",\n",
    "    optim=\"sgd\",\n",
    "    resume_from_checkpoint=True\n",
    ")\n",
    "\n",
    "trainer = BatchEvalTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=sorted_test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45558446-3cbd-4561-98f1-72660e9a7e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvladimirshilonosov2\u001b[0m (\u001b[33mvladimirshilonosov2-itmo\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/vshilonosov/custom-llm-training/wandb/run-20250323_031226-x9dilp3n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vladimirshilonosov2-itmo/huggingface/runs/x9dilp3n' target=\"_blank\">Llama-ru-220M_pretraining</a></strong> to <a href='https://wandb.ai/vladimirshilonosov2-itmo/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vladimirshilonosov2-itmo/huggingface' target=\"_blank\">https://wandb.ai/vladimirshilonosov2-itmo/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vladimirshilonosov2-itmo/huggingface/runs/x9dilp3n' target=\"_blank\">https://wandb.ai/vladimirshilonosov2-itmo/huggingface/runs/x9dilp3n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vshilonosov/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='10257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   33/10257 17:02 < 93:42:21, 0.03 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df56714a-27e6-4037-ab81-f2ba6739428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
